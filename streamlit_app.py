"""
================================================================================
  Data Loader & Explorer - Streamlit in Snowflake App
  è¤‡æ•°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¯¾å¿œã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ï¼†æ¢ç´¢ã‚¢ãƒ—ãƒª
================================================================================

ã€å¯¾å¿œãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘
- CSV / TSVï¼ˆã‚«ãƒ³ãƒãƒ»ã‚¿ãƒ–åŒºåˆ‡ã‚Šï¼‰
- JSON
- Parquet
- Avro
- ORC

ã€æ©Ÿèƒ½ã€‘
- ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ â†’ ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
- ã‚«ãƒ©ãƒ é¸æŠ & WHEREæ¡ä»¶æŒ‡å®š
- SQLç”Ÿæˆ & å®Ÿè¡Œ
- çµæœè¡¨ç¤º & ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

================================================================================
"""

import streamlit as st
from snowflake.snowpark.context import get_active_session
from snowflake.snowpark.types import *
import pandas as pd
import io
import json
from datetime import datetime

# =========================================================
# ãƒšãƒ¼ã‚¸è¨­å®š
# =========================================================
st.set_page_config(
    page_title="Data Loader & Explorer",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Snowflakeã‚»ãƒƒã‚·ãƒ§ãƒ³å–å¾—
@st.cache_resource
def get_snowflake_session():
    return get_active_session()

session = get_snowflake_session()

# =========================================================
# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
# =========================================================
if 'loaded_table' not in st.session_state:
    st.session_state.loaded_table = None
if 'query_history' not in st.session_state:
    st.session_state.query_history = []
if 'current_mode' not in st.session_state:
    st.session_state.current_mode = 'upload'

# =========================================================
# å…±é€šé–¢æ•°
# =========================================================
@st.cache_data(ttl=300)
def get_databases():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¸€è¦§ã‚’å–å¾—"""
    result = session.sql("SHOW DATABASES").collect()
    return [row['name'] for row in result]

@st.cache_data(ttl=300)
def get_schemas(database):
    """ã‚¹ã‚­ãƒ¼ãƒä¸€è¦§ã‚’å–å¾—"""
    result = session.sql(f"SHOW SCHEMAS IN DATABASE {database}").collect()
    return [row['name'] for row in result]

@st.cache_data(ttl=60)
def get_tables(database, schema):
    """ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ã‚’å–å¾—ï¼ˆä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’é™¤å¤–ï¼‰"""
    result = session.sql(f"SHOW TABLES IN {database}.{schema}").collect()
    # SNOWPARK_TEMP ã‚„ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’é™¤å¤–
    excluded_prefixes = ('SNOWPARK_TEMP', 'TEMP_', '_TEMP', 'TMP_')
    return [row['name'] for row in result 
            if not row['name'].upper().startswith(excluded_prefixes)]

def get_table_columns(database, schema, table):
    """ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚«ãƒ©ãƒ æƒ…å ±ã‚’å–å¾—"""
    result = session.sql(f"DESCRIBE TABLE {database}.{schema}.{table}").collect()
    return [(row['name'], row['type']) for row in result]

@st.cache_data(ttl=600, show_spinner=False)
def get_table_descriptions_with_ai(database: str, schema: str, table: str):
    """AI_GENERATE_TABLE_DESCã‚’ä½¿ã£ã¦ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ»ã‚«ãƒ©ãƒ èª¬æ˜ã‚’ç”Ÿæˆï¼ˆ10åˆ†ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰"""
    import re
    
    full_table_name = f"{database}.{schema}.{table}"
    
    # AI_GENERATE_TABLE_DESCã‚’è©¦è¡Œ
    try:
        # AI_GENERATE_TABLE_DESCã®ã‚¯ã‚¨ãƒªï¼ˆæ—¥æœ¬èªã§å›ç­”ã‚’è¦æ±‚ï¼‰
        ai_query = f"""
        SELECT SNOWFLAKE.CORTEX.AI_GENERATE_TABLE_DESC(
            TABLE_NAME => '{full_table_name}',
            LANGUAGE => 'ja'
        )
        """
        ai_result = session.sql(ai_query).collect()
        
        if ai_result and ai_result[0][0]:
            ai_data = json.loads(ai_result[0][0])
            return {
                'table_description': ai_data.get('table_description', ''),
                'column_descriptions': ai_data.get('column_descriptions', {})
            }
    except Exception:
        pass
    
    # AI_GENERATE_TABLE_DESCãŒä½¿ãˆãªã„å ´åˆã€CORTEX.COMPLETEã§ä»£æ›¿å®Ÿè£…
    try:
        # ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ã‚’å–å¾—
        describe_result = session.sql(f"DESCRIBE TABLE {full_table_name}").collect()
        
        if not describe_result:
            return None
        
        # ã‚«ãƒ©ãƒ æƒ…å ±ã‚’ã¾ã¨ã‚ã‚‹
        columns_info = []
        column_names = []
        for row in describe_result:
            columns_info.append(f"{row['name']} ({row['type']})")
            column_names.append(row['name'])
        
        columns_text = "ã€".join(columns_info)
        column_names_json = json.dumps(column_names, ensure_ascii=False)
        
        # AIèª¬æ˜ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt = f"""ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’åˆ†æã—ã€æ—¥æœ¬èªã§èª¬æ˜ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ãƒ†ãƒ¼ãƒ–ãƒ«å: {table}
ã‚«ãƒ©ãƒ æ§‹æˆ: {columns_text}

ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚column_descriptionsã«ã¯å…¨ã¦ã®ã‚«ãƒ©ãƒ ã®èª¬æ˜ã‚’å«ã‚ã¦ãã ã•ã„:
{{"table_description": "ãƒ†ãƒ¼ãƒ–ãƒ«ã®ç”¨é€”ã‚’1-2æ–‡ã§èª¬æ˜", "column_descriptions": {{"ã‚«ãƒ©ãƒ å1": "ãã®ã‚«ãƒ©ãƒ ã®æ„å‘³ã‚„ç”¨é€”", "ã‚«ãƒ©ãƒ å2": "ãã®ã‚«ãƒ©ãƒ ã®æ„å‘³ã‚„ç”¨é€”"}}}}

ã‚«ãƒ©ãƒ åä¸€è¦§: {column_names_json}"""
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
        escaped_prompt = prompt.replace("'", "''")
        cortex_query = f"SELECT SNOWFLAKE.CORTEX.COMPLETE('claude-3-5-sonnet', '{escaped_prompt}')"
        cortex_result = session.sql(cortex_query).collect()
        
        if cortex_result and cortex_result[0][0]:
            response_text = cortex_result[0][0]
            # JSONéƒ¨åˆ†ã‚’æŠ½å‡º
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            
            if json_match:
                json_text = json_match.group(0)
                ai_data = json.loads(json_text)
                
                return {
                    'table_description': ai_data.get('table_description', ''),
                    'column_descriptions': ai_data.get('column_descriptions', {})
                }
    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯Noneã‚’è¿”ã™ï¼ˆUIã§ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼‰
        pass
    
    return None

@st.cache_data(ttl=600, show_spinner=False)
def get_table_columns_with_descriptions(database: str, schema: str, table: str):
    """ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚«ãƒ©ãƒ åã€ãƒ‡ãƒ¼ã‚¿å‹ã€AIç”Ÿæˆèª¬æ˜ã‚’å–å¾—ï¼ˆ10åˆ†ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰"""
    try:
        full_table_name = f"{database}.{schema}.{table}"
        result = session.sql(f"DESCRIBE TABLE {full_table_name}").collect()
        columns_with_desc = []
        
        # AIèª¬æ˜ã‚’å–å¾—
        ai_descriptions = get_table_descriptions_with_ai(database, schema, table)
        
        for row in result:
            col_name = row['name']
            col_type = row['type']
            
            # ã‚µãƒ³ãƒ—ãƒ«å€¤ã‚’å–å¾—
            sample_text = ""
            try:
                sample_query = f'SELECT DISTINCT "{col_name}" FROM {full_table_name} WHERE "{col_name}" IS NOT NULL LIMIT 3'
                sample_result = session.sql(sample_query).collect()
                
                if sample_result:
                    sample_values = [str(r[0])[:30] for r in sample_result]  # å„å€¤ã¯30æ–‡å­—ã¾ã§
                    sample_text = "ã€".join(sample_values[:3])
                else:
                    sample_text = "ï¼ˆãƒ‡ãƒ¼ã‚¿ãªã—ï¼‰"
            except Exception:
                sample_text = "ï¼ˆå–å¾—ã‚¨ãƒ©ãƒ¼ï¼‰"
            
            # AIèª¬æ˜ã‚’å–å¾—
            ai_desc = ""
            if ai_descriptions and ai_descriptions.get('column_descriptions', {}).get(col_name):
                ai_desc = ai_descriptions['column_descriptions'][col_name]
            
            columns_with_desc.append({
                'name': col_name,
                'type': col_type,
                'ai_description': ai_desc,
                'sample_values': sample_text
            })
        
        table_description = ai_descriptions.get('table_description', '') if ai_descriptions else ''
        return columns_with_desc, table_description
        
    except Exception as e:
        return [], ""

def add_query_history(query: str, status: str, rows: int = 0):
    """ã‚¯ã‚¨ãƒªå±¥æ­´ã«è¿½åŠ """
    record = {
        'timestamp': datetime.now(),
        'query': query[:100] + '...' if len(query) > 100 else query,
        'status': status,
        'rows': rows
    }
    st.session_state.query_history.insert(0, record)
    st.session_state.query_history = st.session_state.query_history[:10]

def detect_file_type(uploaded_file):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã‚’æ¤œå‡º"""
    filename = uploaded_file.name.lower()
    if filename.endswith('.csv'):
        return 'CSV'
    elif filename.endswith('.tsv'):
        return 'TSV'
    elif filename.endswith('.json') or filename.endswith('.jsonl'):
        return 'JSON'
    elif filename.endswith('.parquet'):
        return 'PARQUET'
    elif filename.endswith('.avro'):
        return 'AVRO'
    elif filename.endswith('.orc'):
        return 'ORC'
    else:
        return 'UNKNOWN'

# =========================================================
# ã‚µã‚¤ãƒ‰ãƒãƒ¼
# =========================================================
def render_sidebar():
    st.sidebar.header("ğŸ§­ ãƒ¡ãƒ‹ãƒ¥ãƒ¼")
    
    # ãƒ¢ãƒ¼ãƒ‰é¸æŠ
    st.sidebar.markdown("### ğŸ“‹ æ©Ÿèƒ½é¸æŠ")
    
    if st.sidebar.button("ğŸ“¤ ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", use_container_width=True, 
                         type="primary" if st.session_state.current_mode == 'upload' else "secondary"):
        st.session_state.current_mode = 'upload'
        st.rerun()
    
    if st.sidebar.button("ğŸ” ãƒ‡ãƒ¼ã‚¿æ¢ç´¢", use_container_width=True,
                         type="primary" if st.session_state.current_mode == 'explore' else "secondary"):
        st.session_state.current_mode = 'explore'
        st.rerun()
    
    st.sidebar.markdown("---")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ»ã‚¹ã‚­ãƒ¼ãƒé¸æŠ
    st.sidebar.markdown("### âš™ï¸ æ¥ç¶šè¨­å®š")
    
    databases = get_databases()
    selected_db = st.sidebar.selectbox(
        "ğŸ“ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹",
        databases,
        index=databases.index("FROSTYFRIDAY") if "FROSTYFRIDAY" in databases else 0,
        key="sidebar_db"
    )
    
    schemas = get_schemas(selected_db)
    selected_schema = st.sidebar.selectbox(
        "ğŸ“‚ ã‚¹ã‚­ãƒ¼ãƒ",
        schemas,
        index=schemas.index("WEEK80") if "WEEK80" in schemas else 0,
        key="sidebar_schema"
    )
    
    st.sidebar.info(f"ğŸ“ **ç¾åœ¨ã®å ´æ‰€**\n\n`{selected_db}.{selected_schema}`")
    
    st.sidebar.markdown("---")
    
    # å¯¾å¿œãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    st.sidebar.markdown("### ğŸ“ å¯¾å¿œãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ")
    st.sidebar.markdown("""
    - âœ… CSV / TSV
    - âœ… JSON / JSONL
    - âœ… Parquet
    - âœ… Avro
    - âœ… ORC
    """)
    
    # æœ€è¿‘ã®å±¥æ­´
    if st.session_state.query_history:
        st.sidebar.markdown("---")
        st.sidebar.markdown("### ğŸ“ æœ€è¿‘ã®å®Ÿè¡Œ")
        for i, h in enumerate(st.session_state.query_history[:3]):
            icon = "âœ…" if h['status'] == "æˆåŠŸ" else "âŒ"
            st.sidebar.caption(f"{icon} {h['timestamp'].strftime('%H:%M')} - {h['rows']}è¡Œ")
    
    return selected_db, selected_schema

# =========================================================
# Part 1: ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
# =========================================================
def render_upload_page(selected_db, selected_schema):
    st.markdown("""
    <div style="text-align: center; padding: 1rem 0;">
        <h1>ğŸ“¤ Data Loader</h1>
        <p style="font-size: 1.1em; color: #666;">ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦Snowflakeãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆ</p>
    </div>
    """, unsafe_allow_html=True)
    
    st.divider()
    
    # 2ã‚«ãƒ©ãƒ ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
    col1, col2 = st.columns([1, 1])
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’äº‹å‰ã«å®šç¾©ï¼ˆå¤‰æ•°ã‚¹ã‚³ãƒ¼ãƒ—å•é¡Œã‚’å›é¿ï¼‰
    delimiter = ","
    encoding = "utf-8"
    has_header = True
    json_lines = False
    
    with col1:
        st.subheader("1ï¸âƒ£ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
        
        uploaded_file = st.file_uploader(
            "å¯¾å¿œå½¢å¼: CSV, TSV, JSON, Parquet, Avro, ORC",
            type=['csv', 'tsv', 'json', 'jsonl', 'parquet', 'avro', 'orc'],
            help="ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—ã¾ãŸã¯ã‚¯ãƒªãƒƒã‚¯ã—ã¦é¸æŠ"
        )
        
        file_type = None
        if uploaded_file is not None:
            try:
                file_type = detect_file_type(uploaded_file)
                
                # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±è¡¨ç¤º
                st.success(f"ğŸ“ **{uploaded_file.name}** ({file_type})")
                
                col_info1, col_info2 = st.columns(2)
                with col_info1:
                    st.metric("ã‚µã‚¤ã‚º", f"{uploaded_file.size / 1024:.1f} KB")
                with col_info2:
                    st.metric("å½¢å¼", file_type)
                
                # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆåˆ¥ã‚ªãƒ—ã‚·ãƒ§ãƒ³
                with st.expander("ğŸ“ èª­ã¿è¾¼ã¿ã‚ªãƒ—ã‚·ãƒ§ãƒ³", expanded=True):
                    if file_type in ['CSV', 'TSV']:
                        delimiter = st.selectbox(
                            "åŒºåˆ‡ã‚Šæ–‡å­—",
                            [",", "\t", ";", "|"],
                            index=0 if file_type == 'CSV' else 1
                        )
                        encoding = st.selectbox(
                            "ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°",
                            ["utf-8", "shift-jis", "cp932"],
                            index=0
                        )
                        has_header = st.checkbox("ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚ã‚Š", value=True)
                    elif file_type == 'JSON':
                        json_lines = st.checkbox(
                            "JSON Lineså½¢å¼ï¼ˆ1è¡Œ1ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼‰", 
                            value=False,
                            help="é€šå¸¸ã®JSONé…åˆ— [{},...] ã®å ´åˆã¯ãƒã‚§ãƒƒã‚¯ä¸è¦ã€‚è‡ªå‹•æ¤œå‡ºã•ã‚Œã¾ã™ã€‚"
                        )
                    else:
                        st.info(f"ğŸ“¦ {file_type}å½¢å¼ã¯è‡ªå‹•çš„ã«èª­ã¿è¾¼ã¾ã‚Œã¾ã™")
            except Exception as upload_err:
                st.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(upload_err)}")
    
    with col2:
        st.subheader("2ï¸âƒ£ ãƒ†ãƒ¼ãƒ–ãƒ«è¨­å®š")
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’å®‰å…¨ã«ç”Ÿæˆ
        default_table_name = "MY_TABLE"
        if uploaded_file is not None:
            try:
                default_table_name = uploaded_file.name.rsplit('.', 1)[0].upper().replace(" ", "_").replace("-", "_")
            except:
                default_table_name = "MY_TABLE"
        
        table_name = st.text_input(
            "ãƒ†ãƒ¼ãƒ–ãƒ«å",
            value=default_table_name,
            help="è‹±æ•°å­—ã¨ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã®ã¿ä½¿ç”¨å¯"
        ).upper().replace(" ", "_")
        
        if_exists = st.radio(
            "åŒåãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆ",
            ["ã‚¨ãƒ©ãƒ¼", "ç½®æ›ï¼ˆDROP & CREATEï¼‰", "è¿½è¨˜ï¼ˆINSERTï¼‰"],
            index=0,
            horizontal=True
        )
        
        if table_name:
            full_table_name = f"{selected_db}.{selected_schema}.{table_name}"
            st.info(f"ğŸ“ ä½œæˆå…ˆ: `{full_table_name}`")
    
    # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨ä½œæˆ
    st.divider()
    
    if uploaded_file is not None and file_type is not None:
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ³ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ
            uploaded_file.seek(0)
            
            if file_type in ['CSV', 'TSV']:
                df = pd.read_csv(
                    uploaded_file,
                    delimiter=delimiter,
                    encoding=encoding,
                    header=0 if has_header else None
                )
            elif file_type == 'JSON':
                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ³ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦å†…å®¹ã‚’èª­ã¿è¾¼ã¿
                uploaded_file.seek(0)
                try:
                    content = uploaded_file.read().decode('utf-8')
                except:
                    uploaded_file.seek(0)
                    content = uploaded_file.read()
                    if isinstance(content, bytes):
                        content = content.decode('utf-8')
                
                # JSONå½¢å¼ã‚’è‡ªå‹•æ¤œå‡ºã—ã¦èª­ã¿è¾¼ã¿
                content_stripped = content.strip()
                
                try:
                    if content_stripped.startswith('['):
                        # JSONé…åˆ—å½¢å¼
                        data = json.loads(content)
                        df = pd.DataFrame(data)
                    elif content_stripped.startswith('{'):
                        # å˜ä¸€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¾ãŸã¯JSON Lines
                        try:
                            # ã¾ãšå˜ä¸€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦è©¦ã™
                            data = json.loads(content)
                            df = pd.DataFrame([data])
                        except:
                            # JSON Linesã¨ã—ã¦è©¦ã™
                            lines = [json.loads(line) for line in content_stripped.split('\n') if line.strip()]
                            df = pd.DataFrame(lines)
                    else:
                        # JSON Linesã¨ã—ã¦è©¦ã™
                        lines = [json.loads(line) for line in content_stripped.split('\n') if line.strip()]
                        df = pd.DataFrame(lines)
                except Exception as json_err:
                    st.error(f"âŒ JSONèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(json_err)}")
                    return
            elif file_type == 'PARQUET':
                df = pd.read_parquet(uploaded_file)
            elif file_type == 'AVRO':
                # Avroã¯fastavroãŒå¿…è¦
                try:
                    import fastavro
                    records = []
                    uploaded_file.seek(0)
                    reader = fastavro.reader(uploaded_file)
                    for record in reader:
                        records.append(record)
                    df = pd.DataFrame(records)
                except ImportError:
                    st.error("âŒ Avroå½¢å¼ã®èª­ã¿è¾¼ã¿ã«ã¯fastavroãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™")
                    return
            elif file_type == 'ORC':
                # ORCã¯pyarrowãŒå¿…è¦
                try:
                    import pyarrow.orc as orc
                    uploaded_file.seek(0)
                    table = orc.read_table(uploaded_file)
                    df = table.to_pandas()
                except ImportError:
                    st.error("âŒ ORCå½¢å¼ã®èª­ã¿è¾¼ã¿ã«ã¯pyarrowãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™")
                    return
            else:
                st.error("âŒ æœªå¯¾å¿œã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™")
                return
            
            st.subheader("3ï¸âƒ£ ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
            
            # çµ±è¨ˆæƒ…å ±
            col_stat1, col_stat2, col_stat3, col_stat4 = st.columns(4)
            with col_stat1:
                st.metric("è¡Œæ•°", f"{len(df):,}")
            with col_stat2:
                st.metric("åˆ—æ•°", len(df.columns))
            with col_stat3:
                st.metric("ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼", file_type)
            with col_stat4:
                st.metric("ã‚µã‚¤ã‚º", f"{uploaded_file.size / 1024:.1f} KB")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
            st.dataframe(df.head(10), use_container_width=True, height=300)
            
            # ã‚«ãƒ©ãƒ æƒ…å ±
            with st.expander("ğŸ“‹ ã‚«ãƒ©ãƒ æƒ…å ±"):
                col_df = pd.DataFrame({
                    'ã‚«ãƒ©ãƒ å': df.columns,
                    'ãƒ‡ãƒ¼ã‚¿å‹': df.dtypes.astype(str),
                    'Nullæ•°': df.isnull().sum().values,
                    'ã‚µãƒ³ãƒ—ãƒ«å€¤': [str(df[col].iloc[0])[:50] if len(df) > 0 else '' for col in df.columns]
                })
                st.dataframe(col_df, use_container_width=True)
            
            st.divider()
            
            # ä½œæˆãƒœã‚¿ãƒ³
            st.subheader("4ï¸âƒ£ ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ")
            
            if st.button("ğŸš€ ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆ", type="primary", use_container_width=True):
                with st.spinner("ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆä¸­..."):
                    try:
                        # å®Œå…¨ä¿®é£¾åã‚’ä½¿ç”¨ï¼ˆUSE DATABASE/SCHEMAã¯Streamlitã§ã¯ä½¿ç”¨ä¸å¯ï¼‰
                        full_table_path = f"{selected_db}.{selected_schema}.{table_name}"
                        
                        if if_exists == "ç½®æ›ï¼ˆDROP & CREATEï¼‰":
                            session.sql(f"DROP TABLE IF EXISTS {full_table_path}").collect()
                        
                        snowpark_df = session.create_dataframe(df)
                        
                        if if_exists == "è¿½è¨˜ï¼ˆINSERTï¼‰":
                            snowpark_df.write.mode("append").save_as_table(full_table_path)
                        else:
                            snowpark_df.write.mode("errorifexists").save_as_table(full_table_path)
                        
                        st.success(f"âœ… ãƒ†ãƒ¼ãƒ–ãƒ« `{full_table_name}` ã‚’ä½œæˆã—ã¾ã—ãŸï¼")
                        st.session_state.loaded_table = full_table_name
                        
                        add_query_history(f"CREATE TABLE {full_table_path}", "æˆåŠŸ", len(df))
                        
                        # ä½œæˆå¾Œã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
                        st.code(f"SELECT * FROM {full_table_name} LIMIT 10;", language="sql")
                        
                        result_df = session.sql(f"SELECT * FROM {full_table_path} LIMIT 10").to_pandas()
                        st.dataframe(result_df, use_container_width=True)
                        
                        st.snow()
                        
                        # ãƒ‡ãƒ¼ã‚¿æ¢ç´¢ã¸ã®èª˜å°
                        if st.button("ğŸ” ã“ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ç´¢ã™ã‚‹"):
                            st.session_state.current_mode = 'explore'
                            st.rerun()
                        
                    except Exception as e:
                        add_query_history(f"CREATE TABLE {table_name}", "å¤±æ•—", 0)
                        if "already exists" in str(e).lower():
                            st.error(f"âŒ ãƒ†ãƒ¼ãƒ–ãƒ« `{table_name}` ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™")
                        else:
                            st.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        except Exception as e:
            st.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
    else:
        st.info("ğŸ‘† ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")

# =========================================================
# Part 2: ãƒ‡ãƒ¼ã‚¿æ¢ç´¢æ©Ÿèƒ½
# =========================================================
def render_explore_page(selected_db, selected_schema):
    st.markdown("""
    <div style="text-align: center; padding: 1rem 0;">
        <h1>ğŸ” Data Explorer</h1>
        <p style="font-size: 1.1em; color: #666;">ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’é¸æŠã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’æ¢ç´¢ãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰</p>
    </div>
    """, unsafe_allow_html=True)
    
    st.divider()
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«é¸æŠ
    st.subheader("1ï¸âƒ£ ãƒ†ãƒ¼ãƒ–ãƒ«é¸æŠ")
    
    tables = get_tables(selected_db, selected_schema)
    
    if not tables:
        st.warning("ğŸ“­ ã“ã®ã‚¹ã‚­ãƒ¼ãƒã«ã¯ãƒ†ãƒ¼ãƒ–ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ˆã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        if st.button("ğŸ“¤ ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¸"):
            st.session_state.current_mode = 'upload'
            st.rerun()
        return
    
    # æœ€è¿‘ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ†ãƒ¼ãƒ–ãƒ«ãŒã‚ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé¸æŠ
    default_idx = 0
    if st.session_state.loaded_table:
        loaded_table_name = st.session_state.loaded_table.split('.')[-1]
        if loaded_table_name in tables:
            default_idx = tables.index(loaded_table_name)
    
    selected_table = st.selectbox(
        "æ¢ç´¢ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’é¸æŠ",
        tables,
        index=default_idx
    )
    
    if selected_table:
        full_table = f"{selected_db}.{selected_schema}.{selected_table}"
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±
        columns = get_table_columns(selected_db, selected_schema, selected_table)
        row_count = session.sql(f"SELECT COUNT(*) FROM {full_table}").collect()[0][0]
        
        col_info1, col_info2 = st.columns(2)
        with col_info1:
            st.metric("ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°", f"{row_count:,}")
        with col_info2:
            st.metric("ã‚«ãƒ©ãƒ æ•°", len(columns))
        
        # AIèª¬æ˜æ©Ÿèƒ½ã®ãƒˆã‚°ãƒ«
        use_ai_descriptions = st.toggle(
            "ğŸ¤– AIç”Ÿæˆãƒ†ãƒ¼ãƒ–ãƒ«ãƒ»ã‚«ãƒ©ãƒ èª¬æ˜ã‚’è¡¨ç¤º",
            value=True,
            help="AI_GENERATE_TABLE_DESCã‚’ä½¿ã£ã¦ãƒ†ãƒ¼ãƒ–ãƒ«å…¨ä½“ã®æ¦‚è¦ã¨ã‚«ãƒ©ãƒ èª¬æ˜ã‚’è‡ªå‹•ç”Ÿæˆã—ã¾ã™"
        )
        
        # AIèª¬æ˜ã®å–å¾—ã¨è¡¨ç¤º
        if use_ai_descriptions:
            with st.spinner("ğŸ¤– AIèª¬æ˜ã‚’ç”Ÿæˆä¸­..."):
                columns_with_desc, table_description = get_table_columns_with_descriptions(
                    selected_db, selected_schema, selected_table
                )
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«æ¦‚è¦ã‚’è¡¨ç¤º
            if table_description:
                st.info(f"**ğŸ“‹ ãƒ†ãƒ¼ãƒ–ãƒ«æ¦‚è¦**: {table_description}")
            else:
                st.warning("âš ï¸ ãƒ†ãƒ¼ãƒ–ãƒ«æ¦‚è¦ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚Cortex AIã®æ¨©é™ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        
        st.divider()
        
        # ã‚«ãƒ©ãƒ é¸æŠ
        st.subheader("2ï¸âƒ£ è¡¨ç¤ºã‚«ãƒ©ãƒ ã‚’é¸æŠ")
        
        col_names = [c[0] for c in columns]
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã§ã‚«ãƒ©ãƒ é¸æŠã‚’ç®¡ç†
        table_key = f"selected_cols_{selected_table}"
        if table_key not in st.session_state:
            st.session_state[table_key] = []
        
        col_select1, col_select2 = st.columns([3, 1])
        with col_select2:
            if st.button("å…¨é¸æŠ", use_container_width=True):
                st.session_state[table_key] = col_names.copy()
                st.rerun()
            if st.button("å…¨è§£é™¤", use_container_width=True):
                st.session_state[table_key] = []
                st.rerun()
        
        with col_select1:
            selected_columns = st.multiselect(
                "ã‚«ãƒ©ãƒ ã‚’é¸æŠï¼ˆç©ºã®å ´åˆã¯å…¨ã‚«ãƒ©ãƒ ï¼‰",
                col_names,
                default=st.session_state[table_key],
                help="è¤‡æ•°é¸æŠå¯èƒ½",
                key=f"multiselect_{selected_table}"
            )
            # é¸æŠçŠ¶æ…‹ã‚’ä¿å­˜
            st.session_state[table_key] = selected_columns
        
        # ã‚«ãƒ©ãƒ æƒ…å ±ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆAIèª¬æ˜ä»˜ãï¼‰
        with st.expander("ğŸ“‹ ã‚«ãƒ©ãƒ æƒ…å ±", expanded=use_ai_descriptions):
            if use_ai_descriptions and columns_with_desc:
                # AIèª¬æ˜ä»˜ãã®ã‚«ãƒ©ãƒ æƒ…å ±ã‚’è¡¨ç¤º
                display_data = []
                for col_info in columns_with_desc:
                    display_data.append({
                        'ã‚«ãƒ©ãƒ å': col_info['name'],
                        'ãƒ‡ãƒ¼ã‚¿å‹': col_info['type'],
                        'AIèª¬æ˜': col_info.get('ai_description', ''),
                        'ã‚µãƒ³ãƒ—ãƒ«å€¤': col_info.get('sample_values', '')
                    })
                
                col_df = pd.DataFrame(display_data)
                
                # ã‚«ãƒ©ãƒ è¨­å®š
                column_config = {
                    "ã‚«ãƒ©ãƒ å": st.column_config.TextColumn("ã‚«ãƒ©ãƒ å", width="medium"),
                    "ãƒ‡ãƒ¼ã‚¿å‹": st.column_config.TextColumn("ãƒ‡ãƒ¼ã‚¿å‹", width="small"),
                    "AIèª¬æ˜": st.column_config.TextColumn("AIèª¬æ˜", width="large", help="AI_GENERATE_TABLE_DESCã§ç”Ÿæˆã•ã‚ŒãŸèª¬æ˜"),
                    "ã‚µãƒ³ãƒ—ãƒ«å€¤": st.column_config.TextColumn("ã‚µãƒ³ãƒ—ãƒ«å€¤", width="medium", help="å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«")
                }
                
                st.dataframe(col_df, column_config=column_config, use_container_width=True, hide_index=True)
            else:
                # åŸºæœ¬çš„ãªã‚«ãƒ©ãƒ æƒ…å ±ã®ã¿è¡¨ç¤º
                col_df = pd.DataFrame(columns, columns=['ã‚«ãƒ©ãƒ å', 'ãƒ‡ãƒ¼ã‚¿å‹'])
                st.dataframe(col_df, use_container_width=True)
        
        st.divider()
        
        # WHEREæ¡ä»¶
        st.subheader("3ï¸âƒ£ çµã‚Šè¾¼ã¿æ¡ä»¶ï¼ˆWHEREå¥ï¼‰")
        
        # æ¡ä»¶å…¥åŠ›UI
        where_conditions = []
        
        num_conditions = st.number_input("æ¡ä»¶æ•°", min_value=0, max_value=5, value=0)
        
        for i in range(int(num_conditions)):
            st.markdown(f"**æ¡ä»¶ {i+1}**")
            cond_col1, cond_col2, cond_col3, cond_col4 = st.columns([2, 1, 2, 1])
            
            with cond_col1:
                cond_column = st.selectbox(
                    "ã‚«ãƒ©ãƒ ",
                    col_names,
                    key=f"cond_col_{i}"
                )
            
            with cond_col2:
                cond_operator = st.selectbox(
                    "æ¼”ç®—å­",
                    ["=", "!=", ">", ">=", "<", "<=", "LIKE", "IN", "IS NULL", "IS NOT NULL"],
                    key=f"cond_op_{i}"
                )
            
            with cond_col3:
                if cond_operator in ["IS NULL", "IS NOT NULL"]:
                    cond_value = ""
                else:
                    cond_value = st.text_input(
                        "å€¤",
                        key=f"cond_val_{i}",
                        help="æ–‡å­—åˆ—ã¯ 'value' ã®ã‚ˆã†ã«ã‚¯ã‚©ãƒ¼ãƒˆã§å›²ã‚“ã§ãã ã•ã„"
                    )
            
            with cond_col4:
                if i < int(num_conditions) - 1:
                    cond_logic = st.selectbox(
                        "è«–ç†",
                        ["AND", "OR"],
                        key=f"cond_logic_{i}"
                    )
                else:
                    cond_logic = ""
            
            if cond_operator in ["IS NULL", "IS NOT NULL"]:
                where_conditions.append({
                    'column': cond_column,
                    'operator': cond_operator,
                    'value': '',
                    'logic': cond_logic
                })
            elif cond_value:
                where_conditions.append({
                    'column': cond_column,
                    'operator': cond_operator,
                    'value': cond_value,
                    'logic': cond_logic
                })
        
        # ã‚«ã‚¹ã‚¿ãƒ WHEREå¥
        custom_where = st.text_area(
            "ã¾ãŸã¯ã€WHEREå¥ã‚’ç›´æ¥å…¥åŠ›ï¼ˆä¸Šè¨˜æ¡ä»¶ã¨ä½µç”¨å¯ï¼‰",
            placeholder="ä¾‹: AMOUNT > 10000 AND STATUS = 'ACTIVE'",
            help="WHERE ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ä¸è¦ã§ã™"
        )
        
        st.divider()
        
        # ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        st.subheader("4ï¸âƒ£ ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
        
        opt_col1, opt_col2, opt_col3 = st.columns(3)
        
        with opt_col1:
            order_by = st.selectbox(
                "ORDER BY",
                ["ãªã—"] + col_names
            )
            if order_by != "ãªã—":
                order_dir = st.radio("ä¸¦ã³é †", ["ASC", "DESC"], horizontal=True)
        
        with opt_col2:
            limit = st.number_input(
                "LIMIT",
                min_value=1,
                max_value=100000,
                value=100
            )
        
        with opt_col3:
            distinct = st.checkbox("DISTINCTï¼ˆé‡è¤‡æ’é™¤ï¼‰")
        
        st.divider()
        
        # SQLç”Ÿæˆ
        st.subheader("5ï¸âƒ£ ç”Ÿæˆã•ã‚ŒãŸSQL")
        
        # ã‚«ãƒ©ãƒ åã‚’ãƒ€ãƒ–ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆã§å›²ã‚€é–¢æ•°ï¼ˆå¤§æ–‡å­—å°æ–‡å­—ã‚’ä¿æŒï¼‰
        def quote_column(col_name):
            return f'"{col_name}"'
        
        # SELECTå¥
        if selected_columns:
            select_clause = ", ".join([quote_column(c) for c in selected_columns])
        else:
            select_clause = "*"
        
        if distinct:
            select_clause = f"DISTINCT {select_clause}"
        
        # WHEREå¥ã®æ§‹ç¯‰
        where_parts = []
        for cond in where_conditions:
            quoted_col = quote_column(cond['column'])
            if cond['operator'] in ["IS NULL", "IS NOT NULL"]:
                where_parts.append(f"{quoted_col} {cond['operator']}")
            else:
                where_parts.append(f"{quoted_col} {cond['operator']} {cond['value']}")
            if cond['logic']:
                where_parts.append(cond['logic'])
        
        where_clause = " ".join(where_parts)
        if custom_where:
            if where_clause:
                where_clause = f"({where_clause}) AND ({custom_where})"
            else:
                where_clause = custom_where
        
        # ORDER BYå¥
        order_clause = ""
        if order_by != "ãªã—":
            order_clause = f"ORDER BY {quote_column(order_by)} {order_dir}"
        
        # å®Œå…¨ãªSQL
        sql = f"SELECT {select_clause}\nFROM {full_table}"
        if where_clause:
            sql += f"\nWHERE {where_clause}"
        if order_clause:
            sql += f"\n{order_clause}"
        sql += f"\nLIMIT {limit}"
        
        # SQLè¡¨ç¤ºï¼ˆç·¨é›†å¯èƒ½ï¼‰
        edited_sql = st.text_area(
            "SQLï¼ˆç·¨é›†å¯èƒ½ï¼‰",
            value=sql,
            height=150
        )
        
        # å®Ÿè¡Œãƒœã‚¿ãƒ³
        col_btn1, col_btn2 = st.columns(2)
        
        with col_btn1:
            execute = st.button("â–¶ï¸ SQLã‚’å®Ÿè¡Œ", type="primary", use_container_width=True)
        
        with col_btn2:
            copy_sql = st.button("ğŸ“‹ SQLã‚’ã‚³ãƒ”ãƒ¼", use_container_width=True)
            if copy_sql:
                st.code(edited_sql, language="sql")
                st.info("ğŸ‘† ä¸Šã®ã‚³ãƒ¼ãƒ‰ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ãã ã•ã„")
        
        # å®Ÿè¡Œçµæœ
        if execute:
            st.divider()
            st.subheader("6ï¸âƒ£ å®Ÿè¡Œçµæœ")
            
            with st.spinner("ã‚¯ã‚¨ãƒªå®Ÿè¡Œä¸­..."):
                try:
                    start_time = datetime.now()
                    result_df = session.sql(edited_sql).to_pandas()
                    execution_time = (datetime.now() - start_time).total_seconds()
                    
                    add_query_history(edited_sql, "æˆåŠŸ", len(result_df))
                    
                    # çµæœæƒ…å ±
                    res_col1, res_col2, res_col3 = st.columns(3)
                    with res_col1:
                        st.metric("å–å¾—è¡Œæ•°", f"{len(result_df):,}")
                    with res_col2:
                        st.metric("ã‚«ãƒ©ãƒ æ•°", len(result_df.columns))
                    with res_col3:
                        st.metric("å®Ÿè¡Œæ™‚é–“", f"{execution_time:.2f}ç§’")
                    
                    # çµæœè¡¨ç¤º
                    st.dataframe(result_df, use_container_width=True, height=400)
                    
                    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
                    st.markdown("### ğŸ“¥ ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
                    
                    dl_col1, dl_col2, dl_col3 = st.columns(3)
                    
                    with dl_col1:
                        csv_data = result_df.to_csv(index=False)
                        st.download_button(
                            label="ğŸ“„ CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=csv_data,
                            file_name=f"{selected_table}_export.csv",
                            mime="text/csv",
                            use_container_width=True
                        )
                    
                    with dl_col2:
                        json_data = result_df.to_json(orient='records', force_ascii=False, indent=2)
                        st.download_button(
                            label="ğŸ“‹ JSONã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=json_data,
                            file_name=f"{selected_table}_export.json",
                            mime="application/json",
                            use_container_width=True
                        )
                    
                    with dl_col3:
                        # TSVï¼ˆã‚¿ãƒ–åŒºåˆ‡ã‚Šï¼‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ - Excelã§é–‹ã‘ã‚‹å½¢å¼
                        tsv_data = result_df.to_csv(index=False, sep='\t')
                        st.download_button(
                            label="ğŸ“Š TSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=tsv_data,
                            file_name=f"{selected_table}_export.tsv",
                            mime="text/tab-separated-values",
                            use_container_width=True,
                            help="ã‚¿ãƒ–åŒºåˆ‡ã‚Šå½¢å¼ - Excelã§é–‹ã‘ã¾ã™"
                        )
                    
                    st.success("âœ… ã‚¯ã‚¨ãƒªãŒæ­£å¸¸ã«å®Ÿè¡Œã•ã‚Œã¾ã—ãŸ")
                    
                except Exception as e:
                    add_query_history(edited_sql, "å¤±æ•—", 0)
                    st.error(f"âŒ ã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")

# =========================================================
# ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
# =========================================================
def main():
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    selected_db, selected_schema = render_sidebar()
    
    # ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ãŸãƒšãƒ¼ã‚¸è¡¨ç¤º
    if st.session_state.current_mode == 'upload':
        render_upload_page(selected_db, selected_schema)
    else:
        render_explore_page(selected_db, selected_schema)
    
    # ãƒ•ãƒƒã‚¿ãƒ¼
    st.markdown("---")
    st.markdown(
        "<div style='text-align: center; color: #666; padding: 0.5rem;'>"
        "ğŸ“Š Data Loader & Explorer - Frosty Friday Week 80 Demo</div>",
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()
